# The basic RNN tf.nn.rnn accepts a list of inputs of shape [batch_size, input_size]
# Dynamic RNN takes [batch_size, truncated_backprop_length, input_size]
# State series have shape [batch_size*truncated_backprop_length, state_size]
# Logits have shape [batch_size*truncated_backprop_length, num_classes]
# Labels have shape [batch_size*truncated_backprop_length]
# W2 have shape [state_size, num_classes]
# b2 have shape [1, num_classes]

import numpy as np
import tensorflow as tf
import codecs, sys, os

#==============================================================================
# Model parameters
#==============================================================================
class params():
    def __init__(self, num_layers=3, batch_size=64, seq_length=30, keep_prob=0.8,
                 state_size=128, learning_rate=0.01, num_epochs=101, log_step=1,
                 sample_step=50, sample_text_size=500, save_step=100,
                 checkpoint_dir='ckpt', first_sequence='terror '):
        # Number of layers in a network (default 3)
        self.num_layers = num_layers 
        # Batch size (default 64)
        self.batch_size = batch_size 
        # Time step to look back at previous characters
        self.seq_length = seq_length
        # Dropout keep probabilities
        self.keep_prob = keep_prob
        # Number of hidden units in LSTM cell
        self.state_size = state_size
        # Optimizer learning rate
        self.learning_rate = learning_rate
        # Number of training epochs
        self.num_epochs = num_epochs
        # After how many epochs report the progress
        self.log_step = log_step
        # After how many epochs print a sample generated by model
        self.sample_step = sample_step
        # How long should the sample be
        self.sample_text_size = sample_text_size
        # How often to save the model
        self.save_step = save_step
        # What the sample should start with
        self.first_sequence = first_sequence
        # Directory where to save the model
        self.checkpoint_dir = checkpoint_dir
        self.checkpoint_path = os.path.join(self.checkpoint_dir, 
                                                 "model.ckpt")
        
    def set_first_sequence(self, first_sequence):
        self.first_sequence = first_sequence

#==============================================================================
# Data preprocessing
#==============================================================================
def preprocess_data(PARAMS, filename):
    
    print('Opening file...')
    with codecs.open(filename, 'r', encoding='utf-8') as f:
        raw_text = f.read().lower()
        raw_text = raw_text.replace('\r', '')
    print('File opened correctly! Preprocessing data...')
        
    chars = sorted(list(set(raw_text)))
    PARAMS.vocab_size = len(chars) # Also a number of classes for classification
    PARAMS.char2idx = {char:idx for idx, char in enumerate(chars)}
    PARAMS.idx2char = {idx:char for idx, char in enumerate(chars)}
    
    indexed_text = [PARAMS.char2idx[char] for char in raw_text]
    
    # Create batches
    chars_per_batch = PARAMS.batch_size * PARAMS.seq_length
    num_batches = len(indexed_text)//chars_per_batch
    # Cut the reminder out
    indexed_text = indexed_text[:num_batches*chars_per_batch]
    # Create input and labels training data. Labels are moved forward by 1 character
    # which network will try to predict
    x = np.array(indexed_text)
    y = np.array(indexed_text[1:] + [indexed_text[0]])
    # Split training data into batches and zip it all together for training
    x_batches = np.split(x.reshape(PARAMS.batch_size, -1), num_batches, axis=1)
    y_batches = np.split(y.reshape(PARAMS.batch_size, -1), num_batches, axis=1)
    PARAMS.batch_data = list(zip(x_batches, y_batches))
    
    print('Preprocessing Finished!')
    return PARAMS

#==============================================================================
# Define model
#==============================================================================
def build_graph(PARAMS, dropout=True):
    
    print('Building tensorflow graph...')
    # Data placeholders
    input_data = tf.placeholder(tf.int32, [None, None])
    targets = tf.placeholder(tf.int32, [None, None])
    
    # num_units = number of units in the LSTM cell
    lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=PARAMS.state_size)
    if dropout:
        drop_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, 
                                                  output_keep_prob=PARAMS.keep_prob)
    stack_cell = tf.nn.rnn_cell.MultiRNNCell([drop_cell] * PARAMS.num_layers)
    
    # The currect batch_size for the input. Might need to change to mini batches
    input_text_shape = tf.shape(input_data)
    init_state = stack_cell.zero_state(input_text_shape[0], tf.float32)
    
    # Look up ids in a list of embedding tensors
    embeddings = tf.get_variable('embedding_matrix', [PARAMS.vocab_size, 
                                                      PARAMS.state_size])
    rnn_inputs = tf.nn.embedding_lookup(params=embeddings, ids=input_data)
    
    # Inputs to dynamic_rnn may be a single tensor where the maximum time is either
    # the first or second dimension or it may be a (nested) tuple of Tensors
    # each of them having matching match and time dimensions.
    # The initial state is of shape [batch_size, cell.state_size] if state_size
    # is an integer
    # The output is either a single tensor having the same number of time steps
    # and batch size or (nested) tuple of such tensors, matching the nested structure
    # The output is a tensor [batch_size, max_time, call.output_size]
    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell=stack_cell, 
                                                 inputs=rnn_inputs,
                                                 initial_state=init_state)
    
    #reshape rnn_outputs and y
    rnn_outputs = tf.reshape(rnn_outputs, [-1, PARAMS.state_size])
    y_reshaped = tf.reshape(targets, [-1])
    
    with tf.variable_scope('softmax'):
        W = tf.get_variable('W', [PARAMS.state_size, PARAMS.vocab_size])
        b = tf.get_variable('b', [PARAMS.vocab_size], 
                            initializer=tf.constant_initializer(0.0))
            
    logits = tf.matmul(rnn_outputs, W) + b
    predictions = tf.nn.softmax(logits)
    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, 
                                                                         labels=y_reshaped))
    train_step = tf.train.AdamOptimizer(PARAMS.learning_rate).minimize(loss)
    
    print('Tensorflow graph built successfully!')
    return dict(
        input_data = input_data,
        targets = targets,
        init_state = init_state,
        final_state = final_state,
        loss = loss,
        train_step = train_step,
        predictions = predictions,
        saver = tf.train.Saver()
    )
    
def train_network(model, PARAMS, restore=False):
    
    with tf.Session() as sess:
        # Restore a previous model if it exists
        ckpt = tf.train.get_checkpoint_state(PARAMS.checkpoint_dir)
        if ckpt and restore:
            print("Restoring old model parameters from %s..." %
                            ckpt.model_checkpoint_path)
            model['saver'].restore(sess, ckpt.model_checkpoint_path)
            print('Model successfully restored!')
        else:
            print("Creating new model.")        
            # Create a checkpoint directory
            if tf.gfile.Exists(PARAMS.checkpoint_dir):
                tf.gfile.DeleteRecursively(PARAMS.checkpoint_dir)
            tf.gfile.MakeDirs(PARAMS.checkpoint_dir)
            
            sess.run(tf.global_variables_initializer())
        
        print('Initializing training...')
        # Iterate through every epoch and train the model
        for epoch in range(PARAMS.num_epochs):
            state = sess.run(model['init_state'], 
                            {model['input_data']: PARAMS.batch_data[0][0]})
            
            for (x, y) in PARAMS.batch_data:
                train_loss, state, _ = sess.run([model['loss'], 
                                                 model['final_state'], 
                                                 model['train_step']],
                                                 feed_dict={model['input_data']: x,
                                                            model['targets']: y,
                                                            model['init_state']: state})
            if epoch % PARAMS.log_step == 0 and epoch != 0:
                print('Epoch', epoch, 'completed! Loss:', train_loss)
    
            if epoch % PARAMS.sample_step == 0 and epoch != 0:
                print('Building sample...')
                generated_text = PARAMS.first_sequence
                first_input = np.array([[1 for char in PARAMS.first_sequence]])
                state = sess.run(model['init_state'], 
                                {model['input_data']: first_input})
                # For the length of text we want to generate, predict each next
                # character using the network parameters
                for i in range(PARAMS.sample_text_size):
                    input_ = [[PARAMS.char2idx[char] for char in generated_text[-PARAMS.seq_length:]]]
                    
                    # Get prediction
                    probs, state = sess.run([model['predictions'], 
                                             model['final_state']],
                                             feed_dict={model['input_data']: input_,
                                                        model['init_state']: state})
                    # Get the top predicted by our model character index
                    choice_index = np.argmax(probs[-1])
                    # And convert it back into character 
                    predicted_char = PARAMS.idx2char[choice_index]
                    # Concatenate each predicted character together
                    generated_text += predicted_char
                # And print the whole text generated by the model
                print(generated_text)
                
            if epoch % PARAMS.save_step == 0 and epoch != 0:
                model['saver'].save(sess, PARAMS.checkpoint_path)
                print('Model saved!')
            
def generate_sample(model, PARAMS, sample_text_size=None, first_sequence=None):
    
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        model['saver'].restore(sess, PARAMS.checkpoint_path)
        
        # Provide an option for a custom first sequence
        if first_sequence is None:
            generated_text = PARAMS.first_sequence
        else:
            generated_text = first_sequence
            
        first_input = np.array([[1 for char in PARAMS.first_sequence]])
        state = sess.run(model['init_state'], 
                        {model['input_data']: first_input})
        
        # Provie an option for a custom sample text size
        if sample_text_size is None:
            text_size = PARAMS.sample_text_size
        else:
            text_size = sample_text_size
            
        # For the length of text we want to generate, predict each next
        # character using the network parameters
        for i in range(text_size):
            input_ = [[PARAMS.char2idx[char] for char in generated_text[-PARAMS.seq_length:]]]
            
            # Get prediction
            probs, state = sess.run([model['predictions'], 
                                     model['final_state']],
                                     feed_dict={model['input_data']: input_,
                                                model['init_state']: state})
            # Get the top predicted by our model character index
            choice_index = np.argmax(probs[-1])
            # And convert it back into character 
            predicted_char = PARAMS.idx2char[choice_index]
            # Concatenate each predicted character together
            generated_text += predicted_char
        # And print the whole text generated by the model
        print(generated_text)

if __name__ == '__main__':
    
    PARAMS = params(num_layers=3, batch_size=128, seq_length=60, keep_prob=0.7,
                 state_size=128, learning_rate=0.01, num_epochs=101, log_step=1,
                 sample_step=50, sample_text_size=500, save_step=50,
                 first_sequence='terror ')
    PARAMS = preprocess_data(PARAMS, 'metal_lyrics.txt')
    model = build_graph(PARAMS)
    
    if sys.argv[1] == 'train':
        train_network(model, PARAMS, restore=False)
        print('Model has been trained successfully!')
        
    elif sys.argv[1] == 'sample':
        print('Sampling...')
        generate_sample(model, PARAMS)
        
    else:
        print("Please choose train or sample")